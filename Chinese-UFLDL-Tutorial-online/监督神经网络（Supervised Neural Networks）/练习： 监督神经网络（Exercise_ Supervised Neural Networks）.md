# 练习： 监督神经网络（Exercise: Supervised Neural Networks）  

本次练习中，您将训练一个神经网络分类器，并在 MNIST 数据集上对 10 类的手写数字图像分类。神经网络的输出单元与您在 <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression">Softmax 回归</a> 练习中创建的是相同的。仅使用 Softmax 回归的函数去拟合训练集效果并不会很好，其中一个原因是 **欠拟合（$underfitting$）** 。  

相比之下，有着更低偏差（ $bias$ ）的神经网络应能更好地拟合训练集。在 <a href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks">多层神经网络</a> 这一节中，网络参数的梯度是使用反向传播算法对所有参数计算平方误差形式的损失函数（译者注：损失函数，即代价函数）得到的。在本次练习中，需要用到在 Softmax 回归（交叉熵）形式的成本函数，而不是平方误差形式的代价函数。  

神经网络的代价函数与 Softmax 回归的代价函数基本一样。需要注意的是，与从输入数据 $x$ 做预测不同， Softmax 函数把网络 $h_{W,b}(x)$ 的隐含层的最后一层作为输入。其损失函数为：  

$$
\begin{align}
J(\theta) = - \left[ \sum_{i=1}^{m} \sum_{k=1}^{K}  1\left\{y^{(i)} = k\right\} \log \frac{\exp(\theta^{(k)\top} h_{W,b}(x^{(i)}))}{\sum_{j=1}^K \exp(\theta^{(j)\top} h_{W,b}(x)^{(i)}))}\right].
\end{align}
$$  

神经网络和 Softmax 回归在成本函数上的不同，会导致在对输出层 $\delta^{(n_l)}$ 的误差项上二者计算出的值不同。 Softmax （交叉熵）代价为：  

$$
\begin{align}
\delta^{(n_l)} = - \sum_{i=1}^{m}{ \left[ \left( 1\{ y^{(i)} = k\}  - P(y^{(i)} = k | x^{(i)}; \theta) \right) \right]  }
\end{align}
$$

使用这一项，您可以得到计算所有网络参数梯度的完整反向传播算法。  

用前文给出的初学者代码，创建神经网络的前向传播代价函数，并计算其梯度。先前，用 minFunc 优化包来做基于梯度的优化。记得您要对梯度计算的结果进行数值检查。您的实现应该支持多隐含层的神经网络训练。当您在代码实现时，请遵循下面的操作要点：  

- 实现一层隐含层的网络，并做梯度检查。在梯度检查时，您也许会想通过裁剪训练数据的矩阵，来减少输入维度和样本数量。梯度检查时，您可以使用较小数量的隐单元以减少计算时间。
- 实现两个隐含层网络的梯度检查。
- 训练并测试不同的网络架构。您可以实现在一层上有256个隐含单元的隐含层，该结构可以达到在训练集上 100% 的精度。因为有很多参数，所以存在过拟合的风险。通过对不同的层数，隐含层数，以及权重衰减惩罚值的实验，来进一步理解什么样的架构表现最好。您能找到一个优于您最好的单隐含层架构的多隐含层网络吗？
- （可选）扩展您的代码使其支持多种非线性隐含单元的选择（ $S$ 型函数，双曲正切 $tanh$ 函数和 ReLU 函数）。
