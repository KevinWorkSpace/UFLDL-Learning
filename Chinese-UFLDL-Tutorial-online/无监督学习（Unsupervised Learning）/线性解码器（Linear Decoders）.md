## 线性解码器（Linear Decoders）

注：本文参考旧版 UFLDL 中文翻译。  

### 稀疏自编码重述（Sparse Autoencoder Recap）

稀疏自编码器包含 $3$ 层神经元，分别是输入层、隐含层以及输出层。从前面（神经网络）自编码器描述可知，位于神经网络中的神经元都采用相同的激励函数。在注解中，我们修改了自编码器定义，使得某些神经元采用不同的激励函数。这样得到的模型更容易应用，而且模型对参数的变化也更为鲁棒。

回想一下，输出层神经元计算公式如下：

$$
\begin{align}
z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\
a^{(3)} &= f(z^{(3)})
\end{align}
$$

其中 $a(3)$ 是输出。在自编码器中， $a(3)$ 近似重构了输入 $x = a(1)$ 。

S 型激励函数输出范围是 $[0,1]$ ，当 $f(z(3))$ 采用该激励函数时（译注：输出层采用了输出值在 $[0,1]$ 或 $[-1,1]$ 的激励函数，那么要使得自编码成立，那输入层值的范围也是相同才行），就要对输入限制或缩放，使其位于 $[0,1]$ 范围中。一些数据集，比如 MNIST ，能方便将输出缩放到 $[0,1]$ 中，但是很难满足对输入值的要求。比如， PCA 白化处理的输入并不满足 $[0,1]$ 范围要求，也不清楚是否有最好的办法可以将数据缩放到特定范围中。

### 线性解码器（Linear Decoder）

设定 $a(3) = z(3)$ 可以很简单的解决上述问题。从形式上来看，就是输出端使用恒等函数 $f(z) = z$ 作为激励函数，于是有 $a(3) = f(z(3)) = z(3)$ 。我们称该特殊的激励函数为**线性激励函数**（可能称为恒等激励函数更好）。

需要注意，神经网络中隐含层的神经元依然使用 S 型（或者 $tanh$ ）激励函数。这样隐含单元的激励公式为 $\textstyle a^{(2)} = \sigma(W^{(1)}x + b^{(1)})$ ，其中 $\sigma(\cdot)$ 是 S 型函数， $x$ 是输入， $W(1)$ 和 $b(1)$ 分别是隐单元的权重和偏差项。我们仅在输出层中使用线性激励函数。

一个 S 型或 $tanh$ 隐含层以及线性输出层构成的自编码器，我们称为**线性解码器**。

在这个线性解码器模型中， $\hat{x} = a^{(3)} = z^{(3)} = W^{(2)}a + b^{(2)}$ 。因为输出 $\hat{x}$ 是隐单元激励输出的线性函数，改变 $W(2)$ ，可以使输出值 $a(3)$ 大于 $1$ 或者小于 $0$ 。这使得我们可以用实值输入来训练稀疏自编码器，避免预先缩放样本到给定范围。

随着输出单元的激励函数的改变，这个输出单元梯度也相应变化。回顾之前每一个输出单元误差项定义为：
$$
\begin{align}
\delta_i^{(3)}
= \frac{\partial}{\partial z_i} \;\;
        \frac{1}{2} \left\|y - \hat{x}\right\|^2 = - (y_i - \hat{x}_i) \cdot f'(z_i^{(3)})
\end{align}
$$

其中 $y = x$ 是所期望的输出， $\hat{x}$ 是自编码器的输出， $f(\cdot)$ 是激励函数。因为在输出层激励函数为 $f(z) = z$ ，这样 $f'(z) = 1$ ，所以上述公式可以简化为

$$
\begin{align}
\delta_i^{(3)} = - (y_i - \hat{x}_i)
\end{align}
$$

当然，若使用反向传播算法来计算隐含层的误差项时:

$$
\begin{align}
\delta^{(2)} &= \left( (W^{(2)})^T\delta^{(3)}\right) \bullet f'(z^{(2)})
\end{align}
$$

因为隐含层采用一个 S 型（或 $tanh$ ）的激励函数 $f$ ，在上述公式中， $f'(\cdot)$ 依然是 S 型（或 $tanh$ ）函数的导数。
